{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import nltk, re, pprint\n",
    "from nltk import word_tokenize\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### natural Language Processing libraray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Commented out b/c long downloading time, only need to run once\n",
    "# import nltk\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Root to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootdir=\"C:\\\\Users\\\\xyche\\\\Documents\\\\Parsons_Grad\\\\Fall2019\\\\ML\\\\EnronTraining\\\\badeer-r\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### using the OS library, os.walk Recursively walks the given folder and returns the paths of every file or folder within.\n",
    "\n",
    "#### This will show us what's in each folder and how many items are there\n",
    "\n",
    "\n",
    "#### (tutorial :https://www.pythonforengineers.com/analysing-the-enron-email-corpus/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xyche\\Documents\\Parsons_Grad\\Fall2019\\ML\\EnronTraining\\badeer-r ['all_documents', 'california', 'cal_articles', 'capx', 'contacts', 'deleted_items', 'discussion_threads', 'dj_articles', 'inbox', 'memo_s', 'move', 'notes_inbox', 'press_releases', 'sent_items', 'var', '_sent_mail'] 0\n",
      "C:\\Users\\xyche\\Documents\\Parsons_Grad\\Fall2019\\ML\\EnronTraining\\badeer-r\\all_documents [] 299\n",
      "C:\\Users\\xyche\\Documents\\Parsons_Grad\\Fall2019\\ML\\EnronTraining\\badeer-r\\california [] 70\n",
      "C:\\Users\\xyche\\Documents\\Parsons_Grad\\Fall2019\\ML\\EnronTraining\\badeer-r\\cal_articles [] 8\n",
      "C:\\Users\\xyche\\Documents\\Parsons_Grad\\Fall2019\\ML\\EnronTraining\\badeer-r\\capx [] 6\n",
      "C:\\Users\\xyche\\Documents\\Parsons_Grad\\Fall2019\\ML\\EnronTraining\\badeer-r\\contacts [] 2\n",
      "C:\\Users\\xyche\\Documents\\Parsons_Grad\\Fall2019\\ML\\EnronTraining\\badeer-r\\deleted_items [] 13\n",
      "C:\\Users\\xyche\\Documents\\Parsons_Grad\\Fall2019\\ML\\EnronTraining\\badeer-r\\discussion_threads [] 277\n",
      "C:\\Users\\xyche\\Documents\\Parsons_Grad\\Fall2019\\ML\\EnronTraining\\badeer-r\\dj_articles [] 7\n",
      "C:\\Users\\xyche\\Documents\\Parsons_Grad\\Fall2019\\ML\\EnronTraining\\badeer-r\\inbox [] 3\n",
      "C:\\Users\\xyche\\Documents\\Parsons_Grad\\Fall2019\\ML\\EnronTraining\\badeer-r\\memo_s [] 10\n",
      "C:\\Users\\xyche\\Documents\\Parsons_Grad\\Fall2019\\ML\\EnronTraining\\badeer-r\\move [] 1\n",
      "C:\\Users\\xyche\\Documents\\Parsons_Grad\\Fall2019\\ML\\EnronTraining\\badeer-r\\notes_inbox [] 115\n",
      "C:\\Users\\xyche\\Documents\\Parsons_Grad\\Fall2019\\ML\\EnronTraining\\badeer-r\\press_releases [] 6\n",
      "C:\\Users\\xyche\\Documents\\Parsons_Grad\\Fall2019\\ML\\EnronTraining\\badeer-r\\sent_items [] 7\n",
      "C:\\Users\\xyche\\Documents\\Parsons_Grad\\Fall2019\\ML\\EnronTraining\\badeer-r\\var [] 1\n",
      "C:\\Users\\xyche\\Documents\\Parsons_Grad\\Fall2019\\ML\\EnronTraining\\badeer-r\\_sent_mail [] 52\n"
     ]
    }
   ],
   "source": [
    "for directory, subdirectory, filenames in  os.walk(rootdir):\n",
    "    print(directory, subdirectory, len(filenames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Emial.parser Libraray\n",
    "#### The Parser API is most useful if you have the entire text of the message in memory, or if the entire message lives in a file on the file system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message-ID: <13314369.1075863586879.JavaMail.evans@thyme>\n",
      "Date: Tue, 29 Aug 2000 11:11:00 -0700 (PDT)\n",
      "From: mary.hain@enron.com\n",
      "To: james.steffes@enron.com, david.delainey@enron.com, john.lavorato@enron.com, \n",
      "\tchristopher.calger@enron.com, tim.belden@enron.com, \n",
      "\tpaul.kaufman@enron.com, sarah.novosel@enron.com, \n",
      "\tdavid.parquet@enron.com, jdasovic@ees.enron.com, \n",
      "\tmona.petrochko@enron.com, kevin.presto@enron.com, \n",
      "\trichard.shapiro@enron.com, steve.kean@enron.com, \n",
      "\tchris.foster@enron.com, robert.badeer@enron.com, \n",
      "\tjeff.richter@enron.com, susan.mara@enron.com\n",
      "Subject: FERC Presentation on California/West Wholesale Market\n",
      "Cc: christi.nicolay@enron.com\n",
      "Mime-Version: 1.0\n",
      "Content-Type: text/plain; charset=us-ascii\n",
      "Content-Transfer-Encoding: 7bit\n",
      "Bcc: christi.nicolay@enron.com\n",
      "X-From: Mary Hain\n",
      "X-To: James D Steffes, David W Delainey, John J Lavorato, Christopher F Calger, Tim Belden, Joe Hartsoe@Enron, Paul Kaufman, Sarah Novosel, David Parquet, jdasovic@ees.enron.com, Mona Petrochko, Kevin M Presto, Richard Shapiro, Steve Kean, Chris H Foster, Robert Badeer, Jeff Richter, Susan J Mara\n",
      "X-cc: Christi Nicolay\n",
      "X-bcc: \n",
      "X-Folder: \\Robert_Badeer_Aug2000\\Notes Folders\\All documents\n",
      "X-Origin: Badeer-R\n",
      "X-FileName: rbadeer.nsf\n",
      "\n",
      "Last Thursday, I made the first attached presentation to the FERC Staff at \n",
      "the power marketer's meeting on the FERC's investigation of the wholesale \n",
      "market in the West (and in particular California).  Ellen Wolf (of Tabors \n",
      "Caramanis) and I created this presentation building on previous presentations \n",
      "by Tim Belden and Dave Parquet.  In the presentation and the meeting we made \n",
      "the following points:\n",
      "There isn't much FERC can do because the cause of the price spikes is not in \n",
      "the wholesale market.  We discouraged FERC from taking any action that would \n",
      "hurt the vibrant wholesale market in the California and the rest of the West \n",
      "as well.\n",
      "High prices logically resulted from scarcity and if the Commission does \n",
      "anything it should (1) investigate whether market power was being exercised \n",
      "by any party and, (2) if necessary to protect the market (while still \n",
      "incenting needed generation) establish a price cap at a scarcity rent level \n",
      "equal to the price at which loads were willing to interrupt.  \n",
      "The IOUs have not properly prepared for the risk of high prices caused by \n",
      "scarcity.  They have failed to hedge and have underscheduled their load, \n",
      "therefore having to fill a large percentage of their load at ISO real time \n",
      "prices.  My analogy was that this was like day trading your retirement fund \n",
      "as an asset allocation scheme.\n",
      "The market would function better if more information was provided to the \n",
      "market.\n",
      "The Commission should do whatever it can to incent participation by load.\n",
      "To see the presentation, detach, save, and view in Powerpoint.  When you do, \n",
      "you will find there are many \"hidden\" slides that were not part of the oral \n",
      "presentation but were provided to Staff in hard copy for additional \n",
      "information.\n",
      "\n",
      "According to the head of the investigation (Scott Miller), the staff got alot \n",
      "more out of this meeting than Staff's previous meetings with the IOUs and the \n",
      "generators.  Based on the numerous phone calls I've been getting, the Staff \n",
      "is looking into the data we provided.  \n",
      "\n",
      "I have also attached a revised version of the presentation that Tim sent to \n",
      "Scott Miller on Friday.  Tim's version conveys the same message but takes a \n",
      "different approach to conveying the message.  On Friday, Tim talked to Scott \n",
      "and answered some additional questions.  Tim said that Enron is in favor of \n",
      "eliminating the mandatory PX buying requirement and would like the IOUs to be \n",
      "able to buy from Enron Online.  He also explained more fully the existence of \n",
      "scarcity .  \n",
      "  5\n"
     ]
    }
   ],
   "source": [
    "from email.parser import Parser\n",
    "\n",
    "file_to_read = \"C:\\\\Users\\\\xyche\\\\Documents\\\\Parsons_Grad\\\\Fall2019\\\\ML\\\\EnronTraining\\\\badeer-r\\\\all_documents\\\\1_\"\n",
    "\n",
    "with open(file_to_read, \"r\") as f:\n",
    "    data = f.read()\n",
    "\n",
    "print(data,5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the library to deconstruct the email structure, we are only interested in the body text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "To:  james.steffes@enron.com, david.delainey@enron.com, john.lavorato@enron.com, \n",
      "\tchristopher.calger@enron.com, tim.belden@enron.com, \n",
      "\tpaul.kaufman@enron.com, sarah.novosel@enron.com, \n",
      "\tdavid.parquet@enron.com, jdasovic@ees.enron.com, \n",
      "\tmona.petrochko@enron.com, kevin.presto@enron.com, \n",
      "\trichard.shapiro@enron.com, steve.kean@enron.com, \n",
      "\tchris.foster@enron.com, robert.badeer@enron.com, \n",
      "\tjeff.richter@enron.com, susan.mara@enron.com\n",
      "\n",
      " From:  mary.hain@enron.com\n",
      "\n",
      " Subject:  FERC Presentation on California/West Wholesale Market\n"
     ]
    }
   ],
   "source": [
    "email = Parser().parsestr(data)\n",
    "\n",
    "print(\"\\nTo: \" , email['to'])\n",
    "print(\"\\n From: \" , email['from'])\n",
    "\n",
    "print(\"\\n Subject: \" , email['subject'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " Body:  Last Thursday, I made the first attached presentation to the FERC Staff at \n",
      "the power marketer's meeting on the FERC's investigation of the wholesale \n",
      "market in the West (and in particular California).  Ellen Wolf (of Tabors \n",
      "Caramanis) and I created this presentation building on previous presentations \n",
      "by Tim Belden and Dave Parquet.  In the presentation and the meeting we made \n",
      "the following points:\n",
      "There isn't much FERC can do because the cause of the price spikes is not in \n",
      "the wholesale market.  We discouraged FERC from taking any action that would \n",
      "hurt the vibrant wholesale market in the California and the rest of the West \n",
      "as well.\n",
      "High prices logically resulted from scarcity and if the Commission does \n",
      "anything it should (1) investigate whether market power was being exercised \n",
      "by any party and, (2) if necessary to protect the market (while still \n",
      "incenting needed generation) establish a price cap at a scarcity rent level \n",
      "equal to the price at which loads were willing to interrupt.  \n",
      "The IOUs have not properly prepared for the risk of high prices caused by \n",
      "scarcity.  They have failed to hedge and have underscheduled their load, \n",
      "therefore having to fill a large percentage of their load at ISO real time \n",
      "prices.  My analogy was that this was like day trading your retirement fund \n",
      "as an asset allocation scheme.\n",
      "The market would function better if more information was provided to the \n",
      "market.\n",
      "The Commission should do whatever it can to incent participation by load.\n",
      "To see the presentation, detach, save, and view in Powerpoint.  When you do, \n",
      "you will find there are many \"hidden\" slides that were not part of the oral \n",
      "presentation but were provided to Staff in hard copy for additional \n",
      "information.\n",
      "\n",
      "According to the head of the investigation (Scott Miller), the staff got alot \n",
      "more out of this meeting than Staff's previous meetings with the IOUs and the \n",
      "generators.  Based on the numerous phone calls I've been getting, the Staff \n",
      "is looking into the data we provided.  \n",
      "\n",
      "I have also attached a revised version of the presentation that Tim sent to \n",
      "Scott Miller on Friday.  Tim's version conveys the same message but takes a \n",
      "different approach to conveying the message.  On Friday, Tim talked to Scott \n",
      "and answered some additional questions.  Tim said that Enron is in favor of \n",
      "eliminating the mandatory PX buying requirement and would like the IOUs to be \n",
      "able to buy from Enron Online.  He also explained more fully the existence of \n",
      "scarcity .  \n",
      " \n"
     ]
    }
   ],
   "source": [
    "print(\"\\n \\n Body: \" , email.get_payload())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### for the sake of time, I'm only working on a small set of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "textdir = \"C:\\\\Users\\\\xyche\\\\Documents\\\\Parsons_Grad\\\\Fall2019\\\\ML\\\\EnronTraining\\\\badeer-r\\\\deleted_items\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Counter library to later on find the most frequently used address/ words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    " from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### email_analyse() takes in 4 parameters: the input email file, a list of all To emails, a list for From emails, and a list that contains the body(text) of the email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def email_analyse(inputfile, to_email_list, from_email_list, email_body):\n",
    "    with open(inputfile, \"r\") as f:\n",
    "        data = f.read()\n",
    " \n",
    "    if email['to']:\n",
    "        email_to = email['to']\n",
    "        email_to = email_to.replace(\"\\n\", \"\")\n",
    "        email_to = email_to.replace(\"\\t\", \"\")\n",
    "        email_to = email_to.replace(\" \", \"\")\n",
    "\n",
    "        email_to = email_to.split(\",\")\n",
    "\n",
    "        for email_to_1 in email_to:\n",
    "            to_email_list.append(email_to_1)\n",
    "\n",
    "    from_email_list.append(email['from'])\n",
    "\n",
    "    email_body.append(email.get_payload())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_email_list = []\n",
    "from_email_list = []\n",
    "email_body = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for directory, subdirectory, filenames in  os.walk(textdir):\n",
    "    for filename in filenames:\n",
    "        email_analyse(os.path.join(directory, filename), to_email_list, from_email_list, email_body )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking most frequent address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "To email adresses: \n",
      "\n",
      "[('james.steffes@enron.com', 13), ('david.delainey@enron.com', 13), ('john.lavorato@enron.com', 13), ('christopher.calger@enron.com', 13), ('tim.belden@enron.com', 13), ('paul.kaufman@enron.com', 13), ('sarah.novosel@enron.com', 13), ('david.parquet@enron.com', 13), ('jdasovic@ees.enron.com', 13), ('mona.petrochko@enron.com', 13)]\n",
      "\n",
      "From email adresses: \n",
      "\n",
      "[('mary.hain@enron.com', 13)]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTo email adresses: \\n\")\n",
    "print(Counter(to_email_list).most_common(10))\n",
    "\n",
    "print(\"\\nFrom email adresses: \\n\")\n",
    "print(Counter(from_email_list).most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We sperate each mailing lists and address and body texts into diffirent files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"to_email_list.txt\", \"w\") as f:\n",
    "    for to_email in to_email_list:\n",
    "        if to_email:\n",
    "            f.write(to_email)\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"from_email_list.txt\", \"w\") as f:\n",
    "    for from_email in from_email_list:\n",
    "        if from_email:\n",
    "            f.write(from_email)\n",
    "            f.write(\"\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"email_body.txt\", \"w\") as f:\n",
    "    for email_bod in email_body:\n",
    "        if email_bod:\n",
    "            f.write(email_bod)\n",
    "            f.write(\"\\n\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the body text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"email_body.txt\", \"r\") as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split them by space into one aaray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Message-ID:', '<13314369.1075863586879.JavaMail.evans@thyme>', 'Date:', 'Tue,', '29', 'Aug', '2000', '11:11:00', '-0700', '(PDT)', 'From:', 'mary.hain@enron.com', 'To:', 'james.steffes@enron.com,', 'david.delainey@enron.com,', 'john.lavorato@enron.com,', 'christopher.calger@enron.com,', 'tim.belden@enron.com,', 'paul.kaufman@enron.com,', 'sarah.novosel@enron.com,', 'david.parquet@enron.com,', 'jdasovic@ees.enron.com,', 'mona.petrochko@enron.com,', 'kevin.presto@enron.com,', 'richard.shapiro@enron.com,', 'steve.kean@enron.com,', 'chris.foster@enron.com,', 'robert.badeer@enron.com,', 'jeff.richter@enron.com,', 'susan.mara@enron.com', 'Subject:', 'FERC', 'Presentation', 'on', 'California/West', 'Wholesale', 'Market', 'Cc:', 'christi.nicolay@enron.com', 'Mime-Version:', '1.0', 'Content-Type:', 'text/plain;', 'charset=us-ascii', 'Content-Transfer-Encoding:', '7bit', 'Bcc:', 'christi.nicolay@enron.com', 'X-From:', 'Mary', 'Hain', 'X-To:', 'James', 'D', 'Steffes,', 'David', 'W', 'Delainey,', 'John', 'J', 'Lavorato,', 'Christopher', 'F', 'Calger,', 'Tim', 'Belden,', 'Joe', 'Hartsoe@Enron,', 'Paul', 'Kaufman,', 'Sarah', 'Novosel,', 'David', 'Parquet,', 'jdasovic@ees.enron.com,', 'Mona', 'Petrochko,', 'Kevin', 'M', 'Presto,', 'Richard', 'Shapiro,', 'Steve', 'Kean,', 'Chris', 'H', 'Foster,', 'Robert', 'Badeer,', 'Jeff', 'Richter,', 'Susan', 'J', 'Mara', 'X-cc:', 'Christi', 'Nicolay', 'X-bcc:', 'X-Folder:', '\\\\Robert_Badeer_Aug2000\\\\Notes', 'Folders\\\\All', 'documents', 'X-Origin:', 'Badeer-R', 'X-FileName:', 'rbadeer.nsf', 'Last', 'Thursday,', 'I', 'made', 'the', 'first', 'attached', 'presentation', 'to', 'the', 'FERC', 'Staff', 'at', 'the', 'power', \"marketer's\", 'meeting', 'on', 'the', \"FERC's\", 'investigation', 'of', 'the', 'wholesale', 'market', 'in', 'the', 'West', '(and', 'in', 'particular', 'California).', 'Ellen', 'Wolf', '(of', 'Tabors', 'Caramanis)', 'and', 'I', 'created', 'this', 'presentation', 'building', 'on', 'previous', 'presentations', 'by', 'Tim', 'Belden', 'and', 'Dave', 'Parquet.', 'In', 'the', 'presentation', 'and', 'the', 'meeting', 'we', 'made', 'the', 'following', 'points:', 'There', \"isn't\", 'much', 'FERC', 'can', 'do', 'because', 'the', 'cause', 'of', 'the', 'price', 'spikes', 'is', 'not', 'in', 'the', 'wholesale', 'market.', 'We', 'discouraged', 'FERC', 'from', 'taking', 'any', 'action', 'that', 'would', 'hurt', 'the', 'vibrant', 'wholesale', 'market', 'in', 'the', 'California', 'and', 'the', 'rest', 'of', 'the', 'West', 'as', 'well.', 'High', 'prices', 'logically', 'resulted', 'from', 'scarcity', 'and', 'if', 'the', 'Commission', 'does', 'anything', 'it', 'should', '(1)', 'investigate', 'whether', 'market', 'power', 'was', 'being', 'exercised', 'by', 'any', 'party', 'and,', '(2)', 'if', 'necessary', 'to', 'protect', 'the', 'market', '(while', 'still', 'incenting', 'needed', 'generation)', 'establish', 'a', 'price', 'cap', 'at', 'a', 'scarcity', 'rent', 'level', 'equal', 'to', 'the', 'price', 'at', 'which', 'loads', 'were', 'willing', 'to', 'interrupt.', 'The', 'IOUs', 'have', 'not', 'properly', 'prepared', 'for', 'the', 'risk', 'of', 'high', 'prices', 'caused', 'by', 'scarcity.', 'They', 'have', 'failed', 'to', 'hedge', 'and', 'have', 'underscheduled', 'their', 'load,', 'therefore', 'having', 'to', 'fill', 'a', 'large', 'percentage', 'of', 'their', 'load', 'at', 'ISO', 'real', 'time', 'prices.', 'My', 'analogy', 'was', 'that', 'this', 'was', 'like', 'day', 'trading', 'your', 'retirement', 'fund', 'as', 'an', 'asset', 'allocation', 'scheme.', 'The', 'market', 'would', 'function', 'better', 'if', 'more', 'information', 'was', 'provided', 'to', 'the', 'market.', 'The', 'Commission', 'should', 'do', 'whatever', 'it', 'can', 'to', 'incent', 'participation', 'by', 'load.', 'To', 'see', 'the', 'presentation,', 'detach,', 'save,', 'and', 'view', 'in', 'Powerpoint.', 'When', 'you', 'do,', 'you', 'will', 'find', 'there', 'are', 'many', '\"hidden\"', 'slides', 'that', 'were', 'not', 'part', 'of', 'the', 'oral', 'presentation', 'but', 'were', 'provided', 'to', 'Staff', 'in', 'hard', 'copy', 'for', 'additional', 'information.', 'According', 'to', 'the', 'head', 'of', 'the', 'investigation', '(Scott', 'Miller),', 'the', 'staff', 'got', 'alot', 'more', 'out', 'of', 'this', 'meeting', 'than', \"Staff's\", 'previous', 'meetings', 'with', 'the', 'IOUs', 'and', 'the', 'generators.', 'Based', 'on', 'the', 'numerous', 'phone', 'calls', \"I've\", 'been', 'getting,', 'the', 'Staff', 'is', 'looking', 'into', 'the', 'data', 'we', 'provided.', 'I', 'have', 'also', 'attached', 'a', 'revised', 'version', 'of', 'the', 'presentation', 'that', 'Tim', 'sent', 'to', 'Scott', 'Miller', 'on', 'Friday.', \"Tim's\", 'version', 'conveys', 'the', 'same', 'message', 'but', 'takes', 'a', 'different', 'approach', 'to', 'conveying', 'the', 'message.', 'On', 'Friday,', 'Tim', 'talked', 'to', 'Scott', 'and', 'answered', 'some', 'additional', 'questions.', 'Tim', 'said', 'that', 'Enron', 'is', 'in', 'favor', 'of', 'eliminating', 'the', 'mandatory', 'PX', 'buying', 'requirement', 'and', 'would', 'like', 'the', 'IOUs', 'to', 'be', 'able', 'to', 'buy', 'from', 'Enron', 'Online.', 'He', 'also', 'explained', 'more', 'fully', 'the', 'existence', 'of', 'scarcity', '.'] 20\n"
     ]
    }
   ],
   "source": [
    "words = data.split( )\n",
    "print(words,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### - split standard contractions, e.g. ``don't`` -> ``do n't`` and ``they'll`` -> ``they 'll``\n",
    "#####    - treat most punctuation characters as separate tokens\n",
    "#####    - split off commas and single quotes, when followed by whitespace\n",
    "#####    - separate periods that appear at the end of line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "import re\n",
    "corpus = data\n",
    "text=''\n",
    "for word in re.findall(r'[a-zA-Z]+', corpus):\n",
    "    text += word + ' '\n",
    "    txt = text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Snowball stemmer help to process words, i.e verb tenses and plurals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message id javamail evans thyme date tue aug pdt from mary hain enron com to james steffes enron com david delainey enron com john lavorato enron com christopher calger enron com tim belden enron com paul kaufman enron com sarah novosel enron com david parquet enron com jdasovic ees enron com mona petrochko enron com kevin presto enron com richard shapiro enron com steve kean enron com chris foster enron com robert badeer enron com jeff richter enron com susan mara enron com subject ferc presentation on california west wholesale market cc christi nicolay enron com mime version content type text plain charset us ascii content transfer encoding bit bcc christi nicolay enron com x from mary hain x to james d steffes david w delainey john j lavorato christopher f calger tim belden joe hartsoe enron paul kaufman sarah novosel david parquet jdasovic ees enron com mona petrochko kevin m presto richard shapiro steve kean chris h foster robert badeer jeff richter susan j mara x cc christi nicolay x bcc x folder robert badeer aug notes folders all documents x origin badeer r x filename rbadeer nsf last thursday i made the first attached presentation to the ferc staff at the power marketer s meeting on the ferc s investigation of the wholesale market in the west and in particular california ellen wolf of tabors caramanis and i created this presentation building on previous presentations by tim belden and dave parquet in the presentation and the meeting we made the following points there isn t much ferc can do because the cause of the price spikes is not in the wholesale market we discouraged ferc from taking any action that would hurt the vibrant wholesale market in the california and the rest of the west as well high prices logically resulted from scarcity and if the commission does anything it should investigate whether market power was being exercised by any party and if necessary to protect the market while still incenting needed generation establish a price cap at a scarcity rent level equal to the price at which loads were willing to interrupt the ious have not properly prepared for the risk of high prices caused by scarcity they have failed to hedge and have underscheduled their load therefore having to fill a large percentage of their load at iso real time prices my analogy was that this was like day trading your retirement fund as an asset allocation scheme the market would function better if more information was provided to the market the commission should do whatever it can to incent participation by load to see the presentation detach save and view in powerpoint when you do you will find there are many hidden slides that were not part of the oral presentation but were provided to staff in hard copy for additional information according to the head of the investigation scott miller the staff got alot more out of this meeting than staff s previous meetings with the ious and the generators based on the numerous phone calls i ve been getting the staff is looking into the data we provided i have also attached a revised version of the presentation that tim sent to scott miller on friday tim s version conveys the same message but takes a different approach to conveying the message on friday tim talked to scott and answered some additional questions tim said that enron is in favor of eliminating the mandatory px buying requirement and would like the ious to be able to buy from enron online he also explained more fully the existence of scarc 5\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "stemmed = stemmer.stem(txt)\n",
    "print(stemmed,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### exmaple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "stemmed2 = stemmer.stem(\"classes\")\n",
    "print(stemmed2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['message', 'id', 'javamail', 'evans', 'thyme', 'date', 'tue', 'aug', 'pdt', 'from', 'mary', 'hain', 'enron', 'com', 'to', 'james', 'steffes', 'enron', 'com', 'david', 'delainey', 'enron', 'com', 'john', 'lavorato', 'enron', 'com', 'christopher', 'calger', 'enron', 'com', 'tim', 'belden', 'enron', 'com', 'paul', 'kaufman', 'enron', 'com', 'sarah', 'novosel', 'enron', 'com', 'david', 'parquet', 'enron', 'com', 'jdasovic', 'ees', 'enron', 'com', 'mona', 'petrochko', 'enron', 'com', 'kevin', 'presto', 'enron', 'com', 'richard', 'shapiro', 'enron', 'com', 'steve', 'kean', 'enron', 'com', 'chris', 'foster', 'enron', 'com', 'robert', 'badeer', 'enron', 'com', 'jeff', 'richter', 'enron', 'com', 'susan', 'mara', 'enron', 'com', 'subject', 'ferc', 'presentation', 'on', 'california', 'west', 'wholesale', 'market', 'cc', 'christi', 'nicolay', 'enron', 'com', 'mime', 'version', 'content', 'type', 'text', 'plain', 'charset', 'us', 'ascii', 'content', 'transfer', 'encoding', 'bit', 'bcc', 'christi', 'nicolay', 'enron', 'com', 'x', 'from', 'mary', 'hain', 'x', 'to', 'james', 'd', 'steffes', 'david', 'w', 'delainey', 'john', 'j', 'lavorato', 'christopher', 'f', 'calger', 'tim', 'belden', 'joe', 'hartsoe', 'enron', 'paul', 'kaufman', 'sarah', 'novosel', 'david', 'parquet', 'jdasovic', 'ees', 'enron', 'com', 'mona', 'petrochko', 'kevin', 'm', 'presto', 'richard', 'shapiro', 'steve', 'kean', 'chris', 'h', 'foster', 'robert', 'badeer', 'jeff', 'richter', 'susan', 'j', 'mara', 'x', 'cc', 'christi', 'nicolay', 'x', 'bcc', 'x', 'folder', 'robert', 'badeer', 'aug', 'notes', 'folders', 'all', 'documents', 'x', 'origin', 'badeer', 'r', 'x', 'filename', 'rbadeer', 'nsf', 'last', 'thursday', 'i', 'made', 'the', 'first', 'attached', 'presentation', 'to', 'the', 'ferc', 'staff', 'at', 'the', 'power', 'marketer', 's', 'meeting', 'on', 'the', 'ferc', 's', 'investigation', 'of', 'the', 'wholesale', 'market', 'in', 'the', 'west', 'and', 'in', 'particular', 'california', 'ellen', 'wolf', 'of', 'tabors', 'caramanis', 'and', 'i', 'created', 'this', 'presentation', 'building', 'on', 'previous', 'presentations', 'by', 'tim', 'belden', 'and', 'dave', 'parquet', 'in', 'the', 'presentation', 'and', 'the', 'meeting', 'we', 'made', 'the', 'following', 'points', 'there', 'isn', 't', 'much', 'ferc', 'can', 'do', 'because', 'the', 'cause', 'of', 'the', 'price', 'spikes', 'is', 'not', 'in', 'the', 'wholesale', 'market', 'we', 'discouraged', 'ferc', 'from', 'taking', 'any', 'action', 'that', 'would', 'hurt', 'the', 'vibrant', 'wholesale', 'market', 'in', 'the', 'california', 'and', 'the', 'rest', 'of', 'the', 'west', 'as', 'well', 'high', 'prices', 'logically', 'resulted', 'from', 'scarcity', 'and', 'if', 'the', 'commission', 'does', 'anything', 'it', 'should', 'investigate', 'whether', 'market', 'power', 'was', 'being', 'exercised', 'by', 'any', 'party', 'and', 'if', 'necessary', 'to', 'protect', 'the', 'market', 'while', 'still', 'incenting', 'needed', 'generation', 'establish', 'a', 'price', 'cap', 'at', 'a', 'scarcity', 'rent', 'level', 'equal', 'to', 'the', 'price', 'at', 'which', 'loads', 'were', 'willing', 'to', 'interrupt', 'the', 'ious', 'have', 'not', 'properly', 'prepared', 'for', 'the', 'risk', 'of', 'high', 'prices', 'caused', 'by', 'scarcity', 'they', 'have', 'failed', 'to', 'hedge', 'and', 'have', 'underscheduled', 'their', 'load', 'therefore', 'having', 'to', 'fill', 'a', 'large', 'percentage', 'of', 'their', 'load', 'at', 'iso', 'real', 'time', 'prices', 'my', 'analogy', 'was', 'that', 'this', 'was', 'like', 'day', 'trading', 'your', 'retirement', 'fund', 'as', 'an', 'asset', 'allocation', 'scheme', 'the', 'market', 'would', 'function', 'better', 'if', 'more', 'information', 'was', 'provided', 'to', 'the', 'market', 'the', 'commission', 'should', 'do', 'whatever', 'it', 'can', 'to', 'incent', 'participation', 'by', 'load', 'to', 'see', 'the', 'presentation', 'detach', 'save', 'and', 'view', 'in', 'powerpoint', 'when', 'you', 'do', 'you', 'will', 'find', 'there', 'are', 'many', 'hidden', 'slides', 'that', 'were', 'not', 'part', 'of', 'the', 'oral', 'presentation', 'but', 'were', 'provided', 'to', 'staff', 'in', 'hard', 'copy', 'for', 'additional', 'information', 'according', 'to', 'the', 'head', 'of', 'the', 'investigation', 'scott', 'miller', 'the', 'staff', 'got', 'alot', 'more', 'out', 'of', 'this', 'meeting', 'than', 'staff', 's', 'previous', 'meetings', 'with', 'the', 'ious', 'and', 'the', 'generators', 'based', 'on', 'the', 'numerous', 'phone', 'calls', 'i', 've', 'been', 'getting', 'the', 'staff', 'is', 'looking', 'into', 'the', 'data', 'we', 'provided', 'i', 'have', 'also', 'attached', 'a', 'revised', 'version', 'of', 'the', 'presentation', 'that', 'tim', 'sent', 'to', 'scott', 'miller', 'on', 'friday', 'tim', 's', 'version', 'conveys', 'the', 'same', 'message', 'but', 'takes', 'a', 'different', 'approach', 'to', 'conveying', 'the', 'message', 'on', 'friday', 'tim', 'talked', 'to', 'scott', 'and', 'answered', 'some', 'additional', 'questions', 'tim', 'said', 'that', 'enron', 'is', 'in', 'favor', 'of', 'eliminating', 'the', 'mandatory', 'px', 'buying', 'requirement', 'and', 'would', 'like', 'the', 'ious', 'to', 'be', 'able', 'to', 'buy', 'from', 'enron', 'online', 'he', 'also', 'explained', 'more', 'fully', 'the', 'existence', 'of', 'scarc'] 2\n"
     ]
    }
   ],
   "source": [
    "tokens = stemmed.split( )\n",
    "print(tokens,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count the most frequent word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 520), ('to', 208), ('of', 156), ('and', 156), ('in', 104), ('market', 91), ('presentation', 78), ('staff', 65), ('on', 65), ('tim', 65), ('that', 65), ('a', 65), ('i', 52), ('ferc', 52), ('at', 52), ('s', 52), ('by', 52), ('was', 52), ('have', 52), ('scarcity', 51)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter \n",
    "Counter = Counter(tokens)\n",
    "most_occur = Counter.most_common(20)\n",
    "print(most_occur)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using bigram to predic the most freqeunt words after two words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after market: [('presentation', 39), ('market', 39), ('ious', 39), ('ferc', 26), ('wholesale', 26), ('west', 26), ('price', 26), ('commission', 26), ('staff', 26), ('first', 13), ('power', 13), ('meeting', 13), ('following', 13), ('cause', 13), ('vibrant', 13), ('california', 13), ('rest', 13), ('risk', 13), ('oral', 13), ('head', 13), ('investigation', 13), ('generators', 13), ('numerous', 13), ('data', 13), ('same', 13), ('message', 13), ('mandatory', 13), ('existence', 13)]\n"
     ]
    }
   ],
   "source": [
    "from nltk import bigrams\n",
    "\n",
    "bgs = bigrams(tokens)\n",
    "market_bgs = filter(lambda item: item[0] == 'the', bgs)\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# before_market = map(lambda item: item[0], market_bgs)\n",
    "after_market = map(lambda item: item[1], market_bgs)\n",
    "\n",
    "# c = Counter(before_market)\n",
    "# c_predict = c.most_common()\n",
    "# print(\"before market: \" , c_predict)\n",
    "\n",
    "c2 = Counter(after_market)\n",
    "c2_predict = c2.most_common()\n",
    "print(\"after market:\" , c2_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### and before two words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before market:  [('of', 78), ('to', 52), ('in', 52), ('and', 39), ('made', 26), ('on', 26), ('at', 13), ('because', 13), ('hurt', 13), ('if', 13), ('protect', 13), ('interrupt', 13), ('for', 13), ('scheme', 13), ('market', 13), ('see', 13), ('miller', 13), ('with', 13), ('getting', 13), ('into', 13), ('conveys', 13), ('conveying', 13), ('eliminating', 13), ('like', 13), ('fully', 13)]\n",
      "after market: []\n"
     ]
    }
   ],
   "source": [
    "from nltk import bigrams\n",
    "\n",
    "bgs = bigrams(tokens)\n",
    "market_bgs = filter(lambda item: item[1] == 'the', bgs)\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "before_market = map(lambda item: item[0], market_bgs)\n",
    "after_market = map(lambda item: item[2], market_bgs)\n",
    "\n",
    "c = Counter(before_market)\n",
    "c_predict = c.most_common()\n",
    "print(\"before market: \" , c_predict)\n",
    "\n",
    "c2 = Counter(after_market)\n",
    "c2_predict = c2.most_common()\n",
    "print(\"after market:\" , c2_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Attempted to using n_gram (5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# from nltk import word_tokenize\n",
    "# from nltk.util import ngrams\n",
    "\n",
    "# bigrams=ngrams(stemmed,2)\n",
    "# print(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# from nltk.util import ngrams\n",
    "# output = list(ngrams(tokens, 5))\n",
    "# print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ### Here is a different approach, using unsupervised learning with word cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_array = np.asarray(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(606, 211)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(strip_accents='unicode', stop_words='english',)\n",
    "\n",
    "X_v = vectorizer.fit_transform(X_array)\n",
    "X_v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "transformer = TfidfTransformer(norm='l1', sublinear_tf=True)\n",
    "X = transformer.fit_transform(X_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='random', max_iter=300,\n",
       "    n_clusters=42, n_init=127, n_jobs=None, precompute_distances='auto',\n",
       "    random_state=27, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "km = KMeans(n_clusters=42, init='random', n_init=127, max_iter=300, tol=0.0001, precompute_distances='auto', random_state=27)\n",
    "km.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  0, 23,  1,  1,  1,\n",
       "        0, 23, 40,  1,  0, 23,  1, 20,  0, 23,  3,  1,  0, 23, 12, 13,  0,\n",
       "       23,  1,  1,  0, 23,  1,  1,  0, 23, 40, 15,  0, 23,  1,  1,  0, 23,\n",
       "        1,  1,  0, 23, 17, 25,  0, 23,  1,  1,  0, 23,  1,  1,  0, 23,  1,\n",
       "        1,  0, 23,  1,  1,  0, 23,  1,  1,  0, 23, 16,  1,  0, 23,  1, 30,\n",
       "       22,  1,  1,  5, 35, 37,  1, 31,  8,  0, 23,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1, 31,  8,  0, 23,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1, 40,  1,  1,  1,  1, 20,  3,  1,  1, 12, 13,  1,  1,\n",
       "        0,  1,  1,  1,  1, 40, 15,  1,  1,  0, 23,  1,  1, 17,  1, 25,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1, 16,  1,  1,  1,  1, 31,  8,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1, 22,  1,  1, 30,  1,  1,  1,  1,\n",
       "        1,  1, 26,  1,  1, 30,  1,  1,  1,  1, 35, 37,  1,  1,  5,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, 22,  4,  1,  1, 10,  1,\n",
       "       12, 13,  1,  7, 15,  1,  1, 22,  1,  1, 26,  1,  1,  1, 27, 28,  1,\n",
       "       29,  1,  1, 30,  1,  1,  1,  1, 32,  1,  1, 33, 34,  1,  1,  1,  1,\n",
       "       35, 37,  1, 38, 30,  1, 41,  1,  1,  1,  1,  1,  1,  1, 35, 37,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  5,  1,  1, 18, 21,  1, 39,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1, 37,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1, 37,  1,  1,  1,  1,  1,  1,  1, 33,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1, 33,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1, 24,  1,  1,  1,  1, 18, 21,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1, 21,  1,  1,  1,  1,  1,  1,  1, 14,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1, 37,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, 37,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, 22,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1, 22,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  9,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1, 26,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1, 22,  1, 12,  1,  1,  9,  1,  1,  1, 12,\n",
       "        1,  1,  1,  1,  1,  6,  1, 19,  1,  1,  1,  1,  1,  1,  6,  1,  1,\n",
       "       12,  1,  1,  9,  1,  1,  1,  1,  1, 12,  1,  1,  0,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1, 11,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, 36,  1,\n",
       "        0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  2])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Term in Each Cluster: \n",
      "Cluster 0:\n",
      " enron\n",
      " wolf\n",
      " filename\n",
      " generators\n",
      " generation\n",
      "Cluster 1:\n",
      " staff\n",
      " badeer\n",
      " load\n",
      " version\n",
      " provided\n",
      "Cluster 2:\n",
      " scarc\n",
      " wolf\n",
      " getting\n",
      " generation\n",
      " fund\n",
      "Cluster 3:\n",
      " christopher\n",
      " wolf\n",
      " getting\n",
      " generation\n",
      " fund\n",
      "Cluster 4:\n",
      " building\n",
      " wolf\n",
      " filename\n",
      " generators\n",
      " generation\n",
      "Cluster 5:\n",
      " west\n",
      " wolf\n",
      " ferc\n",
      " generators\n",
      " generation\n",
      "Cluster 6:\n",
      " message\n",
      " wolf\n",
      " filename\n",
      " generators\n",
      " generation\n",
      "Cluster 7:\n",
      " dave\n",
      " wolf\n",
      " ferc\n",
      " generation\n",
      " fund\n",
      "Cluster 8:\n",
      " nicolay\n",
      " wolf\n",
      " filename\n",
      " generators\n",
      " generation\n",
      "Cluster 9:\n",
      " scott\n",
      " wolf\n",
      " getting\n",
      " generation\n",
      " fund\n",
      "Cluster 10:\n",
      " presentations\n",
      " wolf\n",
      " ferc\n",
      " generation\n",
      " fund\n",
      "Cluster 11:\n",
      " buying\n",
      " wolf\n",
      " filename\n",
      " generators\n",
      " generation\n",
      "Cluster 12:\n",
      " tim\n",
      " wolf\n",
      " generators\n",
      " generation\n",
      " fund\n",
      "Cluster 13:\n",
      " belden\n",
      " wolf\n",
      " filename\n",
      " generators\n",
      " generation\n",
      "Cluster 14:\n",
      " day\n",
      " wolf\n",
      " ferc\n",
      " generation\n",
      " fund\n",
      "Cluster 15:\n",
      " parquet\n",
      " wolf\n",
      " getting\n",
      " generation\n",
      " fund\n",
      "Cluster 16:\n",
      " susan\n",
      " wolf\n",
      " filename\n",
      " generators\n",
      " generation\n",
      "Cluster 17:\n",
      " kevin\n",
      " wolf\n",
      " ferc\n",
      " generators\n",
      " generation\n",
      "Cluster 18:\n",
      " high\n",
      " wolf\n",
      " getting\n",
      " generation\n",
      " fund\n",
      "Cluster 19:\n",
      " takes\n",
      " wolf\n",
      " filename\n",
      " generators\n",
      " generation\n",
      "Cluster 20:\n",
      " lavorato\n",
      " wolf\n",
      " ferc\n",
      " generators\n",
      " generation\n",
      "Cluster 21:\n",
      " prices\n",
      " wolf\n",
      " ferc\n",
      " generation\n",
      " fund\n",
      "Cluster 22:\n",
      " presentation\n",
      " wolf\n",
      " ferc\n",
      " generation\n",
      " fund\n",
      "Cluster 23:\n",
      " com\n",
      " wolf\n",
      " getting\n",
      " generation\n",
      " fund\n",
      "Cluster 24:\n",
      " prepared\n",
      " wolf\n",
      " ferc\n",
      " generation\n",
      " fund\n",
      "Cluster 25:\n",
      " presto\n",
      " wolf\n",
      " ferc\n",
      " generation\n",
      " fund\n",
      "Cluster 26:\n",
      " meeting\n",
      " wolf\n",
      " filename\n",
      " generators\n",
      " generation\n",
      "Cluster 27:\n",
      " following\n",
      " wolf\n",
      " ees\n",
      " generators\n",
      " generation\n",
      "Cluster 28:\n",
      " points\n",
      " wolf\n",
      " ferc\n",
      " generation\n",
      " fund\n",
      "Cluster 29:\n",
      " isn\n",
      " wolf\n",
      " ferc\n",
      " generation\n",
      " fund\n",
      "Cluster 30:\n",
      " ferc\n",
      " ees\n",
      " generators\n",
      " generation\n",
      " fund\n",
      "Cluster 31:\n",
      " christi\n",
      " wolf\n",
      " getting\n",
      " generation\n",
      " fund\n",
      "Cluster 32:\n",
      " cause\n",
      " getting\n",
      " generators\n",
      " generation\n",
      " fund\n",
      "Cluster 33:\n",
      " price\n",
      " wolf\n",
      " ferc\n",
      " generation\n",
      " fund\n",
      "Cluster 34:\n",
      " spikes\n",
      " wolf\n",
      " generators\n",
      " generation\n",
      " fund\n",
      "Cluster 35:\n",
      " wholesale\n",
      " wolf\n",
      " ferc\n",
      " generators\n",
      " generation\n",
      "Cluster 36:\n",
      " buy\n",
      " wolf\n",
      " filename\n",
      " generators\n",
      " generation\n",
      "Cluster 37:\n",
      " market\n",
      " ferc\n",
      " generators\n",
      " generation\n",
      " fund\n",
      "Cluster 38:\n",
      " discouraged\n",
      " wolf\n",
      " ferc\n",
      " generation\n",
      " fund\n",
      "Cluster 39:\n",
      " resulted\n",
      " wolf\n",
      " ferc\n",
      " generation\n",
      " fund\n",
      "Cluster 40:\n",
      " david\n",
      " wolf\n",
      " ferc\n",
      " generation\n",
      " fund\n",
      "Cluster 41:\n",
      " taking\n",
      " wolf\n",
      " filename\n",
      " generators\n",
      " generation\n",
      "Cluster 42:\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 42 is out of bounds for axis 0 with size 42",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-0e5f9ded8035>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Cluster %d:\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mind\u001b[0m \u001b[1;32min\u001b[0m \u001b[0morder_centroids\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m' %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mterms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mind\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 42 is out of bounds for axis 0 with size 42"
     ]
    }
   ],
   "source": [
    "print (\"Top Term in Each Cluster: \")\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i in range(len(km.labels_)-1):\n",
    "    print(\"Cluster %d:\" % i),\n",
    "    for ind in order_centroids[i, :5]:\n",
    "        print (' %s' % terms[ind]),\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The results isn't ideal, now trying with Markvo "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markvo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Markvo chain breaks an input text (training text) into a series of words, then by sliding along them in some fixed sized window, storing the first N words as a prefix and then the N + 1 word as a member of a set to choose from randomly for the suffix.\n",
    "\n",
    "#### There are already existing models with markvo algorithm, below is a moderated version of one (https://rosettacode.org/wiki/Markov_chain_text_generator#Functional)\n",
    "\n",
    "#### However, personally I don't feel this is doing exactly what I'm after, but the result seems great."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment:\n",
      "\n",
      "Friday. Tim's version conveys the same message but takes a different\n",
      "approach to conveying the message. On Friday, Tim talked to Scott and\n",
      "answered some additional questions. Tim said that Enron is in favor of\n",
      "eliminating the mandatory PX buying requirement and would like the IOUs to\n",
      "be able to buy from Enron Online. He also explained more fully the\n",
      "existence of scarcity . Last Thursday, I made the first attached\n",
      "presentation to the FERC Staff at the power marketer's meeting on the\n",
      "FERC's investigation of the wholesale market in the California and the rest\n",
      "of the West as well. High prices logically resulted from scarcity and if\n",
      "the Commission does anything it should (1) investigate whether market power\n",
      "was being exercised by any party and, (2) if necessary to protect the\n",
      "market (while still incenting needed generation) establish a price cap at a\n",
      "scarcity rent level equal to the price at which loads were willing to\n",
      "interrupt. The IOUs have not properly prepared for the risk of high prices\n",
      "caused by scarcity. They have failed to hedge and have underscheduled their\n",
      "load, therefore having to fill a large percentage of their load at ISO real\n",
      "time prices. My analogy was that this was like day trading your retirement\n",
      "fund as an asset allocation scheme. The market would function better if\n",
      "more information was provided to the market. The Commission should do\n",
      "whatever it can to incent participation by load. To see the presentation,\n",
      "detach, save, and view in Powerpoint. When you do, you will find there are\n",
      "many \"hidden\" slides that were not part of the oral presentation but were\n",
      "provided to Staff in hard copy for additional information. According to the\n",
      "head of the investigation (Scott Miller), the staff got alot more out of\n",
      "this meeting than Staff's previous meetings with the IOUs and the\n",
      "generators.\n"
     ]
    }
   ],
   "source": [
    "from os.path import (expanduser)\n",
    "from os import (getcwd)\n",
    " \n",
    "from itertools import (starmap)\n",
    "from functools import (reduce)\n",
    "from random import (choice)\n",
    "from textwrap import (fill)\n",
    " \n",
    " \n",
    "# markovText :: Dict -> [String] -> ([String] -> Bool) -> IO [String]\n",
    "def markovText(dct):\n",
    "    '''nGram-hashed word dict -> opening words -> end condition -> text\n",
    "    '''\n",
    "    # nGram length\n",
    "    n = len(list(dct.keys())[0].split())\n",
    " \n",
    "    # step :: [String] -> [String]\n",
    "    def step(xs):\n",
    "        return xs + [choice(dct[' '.join(xs[-n:])])]\n",
    "    return lambda ws: lambda p: (\n",
    "        until(p)(step)(ws)\n",
    "    )\n",
    " \n",
    " \n",
    "# markovRules :: Int -> [String] -> Dict\n",
    "def markovRules(n):\n",
    "    '''All words in ordered list hashed by\n",
    "       preceding nGrams of length n.\n",
    "    '''\n",
    "    def nGramKey(dct, tpl):\n",
    "        k = ' '.join(list(tpl[:-1]))\n",
    "        dct[k] = (dct[k] if k in dct else []) + [tpl[-1]]\n",
    "        return dct\n",
    "    return lambda ws: reduce(\n",
    "        nGramKey,\n",
    "        nGramsFromWords(1 + n)(ws),\n",
    "        {}\n",
    "    )\n",
    " \n",
    " \n",
    "# TEST ----------------------------------------------------\n",
    "# main :: IO ()\n",
    "def main():\n",
    "    '''Text generation test.'''\n",
    " \n",
    "    nGramLength = 3\n",
    "    dctNGrams = markovRules(nGramLength)(\n",
    "        readFile(getcwd() + '/' + 'email_body.txt').split()\n",
    "    )\n",
    "    print(__doc__ + ':\\n')\n",
    "    print(\n",
    "        fill(\n",
    "            ' '.join(\n",
    "                markovText(dctNGrams)(\n",
    "                    anyNGramWithInitialCap(dctNGrams)\n",
    "                )(sentenceEndAfterMinWords(300))\n",
    "            ),\n",
    "            width=75\n",
    "        )\n",
    "    )\n",
    " \n",
    " \n",
    "# HELPER FUNCTIONS ----------------------------------------\n",
    " \n",
    "# nGramsFromWords :: Int -> [String] -> [Tuple]\n",
    "def nGramsFromWords(n):\n",
    "    '''List of nGrams, of length n, derived\n",
    "       from ordered list of words ws.\n",
    "    '''\n",
    "    return lambda ws: zipWithN(lambda *xs: xs)(\n",
    "        map(lambda i: ws[i:], range(0, n))\n",
    "    )\n",
    " \n",
    " \n",
    "# anyNGramWithInitialCap :: Dict -> [String]\n",
    "def anyNGramWithInitialCap(dct):\n",
    "    '''Random pick from nGrams which\n",
    "       start with capital letters\n",
    "    '''\n",
    "    return choice(list(filter(\n",
    "        lambda k: 1 < len(k) and k[0].isupper() and k[1].islower(),\n",
    "        dct.keys()\n",
    "    ))).split()\n",
    " \n",
    " \n",
    "# sentenceEndAfterMinWords :: Int -> [String] -> Bool\n",
    "def sentenceEndAfterMinWords(n):\n",
    "    '''Predicate :: Sentence punctuation\n",
    "       after minimum word count\n",
    "    '''\n",
    "    return lambda ws: n <= len(ws) and (\n",
    "        ws[-1][-1] in ['.', \"'\", '!', '?']\n",
    "    )\n",
    " \n",
    " \n",
    "# GENERIC -------------------------------------------------\n",
    " \n",
    "# readFile :: FilePath -> IO String\n",
    "def readFile(fp):\n",
    "    '''The contents of any file at the path\n",
    "       derived by expanding any ~ in fp.'''\n",
    "    with open(expanduser(fp), 'r', encoding='utf-8') as f:\n",
    "        return f.read()\n",
    " \n",
    " \n",
    "# until :: (a -> Bool) -> (a -> a) -> a -> a\n",
    "def until(p):\n",
    "    '''The result of repeatedly applying f until p holds.\n",
    "       The initial seed value is x.'''\n",
    "    def go(f, x):\n",
    "        v = x\n",
    "        while not p(v):\n",
    "            v = f(v)\n",
    "        return v\n",
    "    return lambda f: lambda x: go(f, x)\n",
    " \n",
    " \n",
    "# zipWithN :: (a -> b -> ... -> c) -> ([a], [b] ...) -> [c]\n",
    "def zipWithN(f):\n",
    "    '''A new list constructed by the application of f\n",
    "       to each tuple in the zip of an arbitrary\n",
    "       number of existing lists.\n",
    "    '''\n",
    "    return lambda xs: list(\n",
    "        starmap(f, zip(*xs))\n",
    "    )\n",
    " \n",
    " \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
